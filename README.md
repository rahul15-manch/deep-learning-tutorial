# ğŸ§  Deep Learning Fundamentals â€“ Weekly Practice Repository

**Welcome to the Deep Learning Fundamentals repository!**
This project is organized week-by-week to help you learn and implement core machine learning and deep learning concepts such as activation functions, gradient descent, and neural network basics.

## ğŸ“‚ Project Structure
```
Deep-Learning-Fundamentals/
â”‚
â”œâ”€â”€ WEEK-1/
â”‚   â”œâ”€â”€ activation function.ipynb
â”‚   â”œâ”€â”€ Dataset/
â”‚   â”‚   â””â”€â”€ insurance_data.csv
â”‚   â”œâ”€â”€ gradient_descent.ipynb
â”‚   â”œâ”€â”€ loss_function.ipynb
â”‚   â””â”€â”€ mnist.ipynb
â”‚
â””â”€â”€ WEEK-2/
    â”œâ”€â”€ Dataset/
    â”‚   â””â”€â”€ homeprices_banglore.csv
    â””â”€â”€ stochastic vs batch gradient_descent.ipynb
```

## ğŸ“˜ Week 1: Neural Network Basics
###  ğŸ”¹ 1. Activation Functions

- Covers Sigmoid, ReLU, Tanh, and Softmax functions.

- Demonstrates how they transform input signals in a neural network.

- Shows their effect on model learning and convergence.

### ğŸ”¹ 2. Gradient Descent

- Implementation of gradient descent from scratch.

- Explains how weights are updated to minimize loss.

- Visualizes the optimization process.

### ğŸ”¹ 3. Loss Functions

- Explores Mean Squared Error (MSE), Cross Entropy, and Hinge Loss.

- Demonstrates how loss affects model training and accuracy.

### ğŸ”¹ 4. MNIST.ipynb

- Hands-on implementation on the MNIST handwritten digits dataset.

- Uses TensorFlow/Keras for classification.

- Covers model building, training, and evaluation.

### ğŸ”¹ Dataset:

- insurance_data.csv â€” used for demonstrating loss and gradient descent calculations.

## ğŸ“˜ Week 2: Advanced Optimization
### ğŸ”¹ 1. Stochastic vs Batch Gradient Descent

- Compares different types of gradient descent algorithms:

- Batch Gradient Descent

- Stochastic Gradient Descent (SGD)

- Mini-Batch Gradient Descent

- Explains the speedâ€“accuracy trade-off and when to use each.

### ğŸ”¹ Dataset:

- homeprices_banglore.csv â€” used to apply gradient descent on real-world housing price prediction.

## ğŸ§© Concepts Covered

- Activation functions

- Loss functions

- Gradient descent and its variants

- Neural network basics

- Model training and evaluation

- Hands-on dataset-based implementation

## ğŸ§  Tech Stack & Tools Used

<table>
  <tr>
    <td align="center" width="140">
      <img src="https://cdnl.iconscout.com/lottie/premium/thumb/python-animation-gif-download-4079156.gif" width="90"><br>
      <b>Python 3.x</b>
    </td>
    <td align="center" width="140">
      <img src="https://cdn3d.iconscout.com/3d/free/thumb/free-numpy-3d-icon-png-download-7578012.png" width="90"><br>
      <b>NumPy</b>
    </td>
    <td align="center" width="140">
      <img src="https://upload.wikimedia.org/wikipedia/commons/e/ed/Pandas_logo.svg" width="90"><br>
      <b>Pandas</b>
    </td>
    <td align="center" width="140">
      <img src="https://media.geeksforgeeks.org/wp-content/uploads/20210616180348/ezgif1373d33c8462c.gif" width="90"><br>
      <b>Matplotlib / Seaborn</b>
    </td>
  </tr>
  <tr>
    <td align="center" width="140">
      <img src="https://miro.medium.com/1%2ASB-Fu_AySBggAAxq0Q2Wew.gif" width="90"><br>
      <b>TensorFlow</b>
    </td>
    <td align="center" width="140">
      <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Keras_logo.svg/2048px-Keras_logo.svg.png" width="90"><br>
      <b>Keras</b>
    </td>
    <td align="center" width="140">
      <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Scikit_learn_logo_small.svg/2560px-Scikit_learn_logo_small.svg.png" width="90"><br>
      <b>Scikit-learn</b>
    </td>
    <td align="center" width="140">
      <img src="https://uploads-ssl.webflow.com/6473d8d02a3cf26273f2787d/64ae332546c741627b85350d_U1GmZexeuDs_phF2UT8foWfw6JOW72abm6bcPzFc02qvIA4xDB_fALysi5TzwUnkpudHMlb5ssk7sqnypoZwqVct7UFR7-zplAlWLdgH_nXK6Fh_ngOye8mLz82K1lhxz4UN1-BVKsf9EqtxZq_34A.gif" width="90"><br>
      <b>Jupyter Notebook</b>
    </td>
  </tr>
</table>


## ğŸ¯ Learning Outcomes

By the end of this project, I learnt :
- âœ… Understand how neural networks learn using gradient descent.
- âœ… Know the role and effect of different activation and loss functions.
- âœ… Be able to implement and visualize optimization algorithms from scratch.
- âœ… Get hands-on experience with real datasets.

## ğŸ§¾ Author

**ğŸ‘¨â€ğŸ’» Rahul Manchanda**  
ğŸ“ *Data Enthusiast | Machine Learning & Deep Learning*  

[![LinkedIn](https://img.shields.io/badge/LinkedIn-Rahul%20Manchanda-blue?logo=linkedin&logoColor=white)](https://www.linkedin.com/in/rahul-manchanda-3959b120a/)  
[![GitHub](https://img.shields.io/badge/GitHub-rahul15--manch-black?logo=github&logoColor=white)](https://github.com/rahul15-manch)  
[![Instagram](https://img.shields.io/badge/Instagram-%40rahulmanchanda015-%23E4405F?logo=instagram&logoColor=white)](https://www.instagram.com/rahulmanchanda015/?__pwa=1)




